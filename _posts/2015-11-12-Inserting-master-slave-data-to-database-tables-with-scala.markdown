---
layout: post
title: "Inserting master-slave data to database tables with Scala"
date:   2015-11-11 13:06:23 +0000
---
Earlier I discussed the problem of getting a file full of JSON data that is about two gigabyte long into a database. I mentioned that the data in the file that I was
interested in was 'Services'. What I didn't do was explain that as well as the services, there are 'service segments'. Each of the JSON maps that describes a service
also contains a list of ServiceSegments. 

This is looking at the impact of importing both at the same time

#Creating the ServiceSegment table
 {% highlight sql %}
 create table service_segment(
   service_id integer,
   tiploc varchar(40),
   departure char(5),
   arrival char(5)
 );
{% endhighlight %}
This is pretty straight forwards. The 'tiploc' is a location. The departure and arrival times are 24 hour clocks with an optional H on the end marking 'and half a minute'

#Inserting master slave data
In the JSON file the master and slaves are linked by being in a file. In a relational database they have to be linked by an key. 
There is a field called uid (meaning unique id) in the data, but a very quick check shows us that sadly it's not actually as unique as we would like.
The business logic behind the services allows a new service to be described which 'overrides' the services for a scheduled period. 

This means that we need to make our own ID and use it to link the services to the service segments.

It is entirely possible to use a sequence in the database for insertion, and if we are only worrying about a single table this isn't a bad idea. Unfortunately 
we need to know what the ID is going to be in order to insert the service segments (as each service segment needs the id of the service). 

After a little time considering options, I decided to go for simplicity: truncate the database before inserting, and then create the unique ids within Scala. I'd actually
been doing this anyway in the 'SimpleImport' examples 
{% highlight scala %}
    val dateFormatter = new SimpleDateFormat("yyyy-MM-dd")
    val rawLines = Source.fromFile(file).getLines()
    val scheduleLines = rawLines.filter(_.contains("JsonScheduleV1")).
          map(Json.parse(_) \ "JsonScheduleV1")
    val scheduleData = scheduleLines.zipWithIndex.map {
      case (json, id) =>
        def s(j: JsLookupResult) = j.asOpt[String].getOrElse(null)
        def d(j: JsLookupResult) = new java.sql.Date(dateFormatter.
             parse(s(j)).getTime)
        val schedule = json \ "schedule_segment"
        val trainUid = s(json \ "CIF_train_uid")
        val trainCategory = s(schedule \ "CIF_train_category")
        val trainServiceCode = s(schedule \ "CIF_train_service_code")
        val scheduleDayRuns = s(json \ "schedule_days_runs")
        val scheduleStartDate = d(json \ "schedule_start_date")
        val scheduleEndDate = d(json \ "schedule_end_date")
        List(id, trainUid, trainCategory, trainServiceCode, 
              scheduleDayRuns, scheduleStartDate, scheduleEndDate)
    }

    time("just reading the data", scheduleData.foreach(x => x))
    val columnNames = List("id", "uid", "train_category", "train_service_code",
              "scheduleDayRuns", "schedule_start_date", "schedule_end_date")
    withDatasource { implicit dataSource =>
       batchDataToDatabase("serviceForBlog", columnNames, scheduleData)
    }
  }
{% endhighlight %}
As you can see, in this code the id is generated by 'scheduleLines.zipWithIndex'. 
  
#To Domain object or Not To Domain Object
If I was  doing complicated code then the "List(id, trainUid, trainCategory, trainServiceCode, scheduleDayRuns, scheduleStartDate, scheduleEndDate)" should be an 
object of type Service. I'm about to give it some structure by adding a List of what are logically ServiceSegments to it. At a time like this I revisit my
decisions on Domain objects. 

After some thought (and a little thrown away code) I decided that the best thing to do was a couple of 'generic domain model items'. Although I am importing Services
and ServiceSegments, this code knows nothing (and should know nothing) about the domain of Services and ServiceSegments. Instead this code is worrying about MasterSlave databases.

I added the following classes
{% highlight scala %}
class MasterSlaveDefn(val tableName1: String, val columnNames1: List[String],
                      val tableName2: String, val columnNames2: List[String])
class MasterSlaveData(val masterData: List[Any], val slaveData: Seq[List[Any]])
{% endhighlight %}
This allows me to declare the tablename and columns of the master and the slave, and store one lot of master data with any amount of slave data.  
 
#Parsing the ServiceSegments
To do this I just need to modify the above mapping section
{% highlight scala %}
 val scheduleData = scheduleLines.zipWithIndex.map {
      case (json, serviceId) =>
        def s(j: JsLookupResult) = j.asOpt[String].getOrElse(null)
        def d(j: JsLookupResult)= new java.sql.Date(dateFormatter.parse(s(j)).getTime)
        val schedule = json \ "schedule_segment"
        val trainUid = s(json \ "CIF_train_uid")
        val trainCategory = s(schedule \ "CIF_train_category")
        val trainServiceCode = s(schedule \ "CIF_train_service_code")
        val scheduleDayRuns = s(json \ "schedule_days_runs")
        val scheduleStartDate = d(json \ "schedule_start_date")
        val scheduleEndDate = d(json \ "schedule_end_date")

        val serviceSegmentData = (schedule \ "schedule_location").as[Seq[JsObject]].
        map { json =>
	          val tiploc = (json \ "tiploc_code").as[String]
	          val departure = s(json \ "departure")
	          val arrival = s(json \ "arrival")
	          List(serviceId, tiploc, departure, arrival)
	        }

        new MasterSlaveData(List(serviceId, trainUid, trainCategory, trainServiceCode, 
           scheduleDayRuns, scheduleStartDate, scheduleEndDate), serviceSegmentData)
    }
{% endhighlight %}
This is pretty easy code to follow. The only place I had a problem was in the section (schedule \ "schedule_location").as[Seq[JsObject]]. This was needed as I 'know' it contains a 
list of schedule_locations, AKA ServiceSegments. The 'AKA' is another one of those mildly annoying domain language problems when working with a system that is decades old.


#Importing
In the past I have had many difficult to track down issues when I import this sort of data in two goes: once to insert the services, and the second
to insert the service segments. I am quite keen to insert them both at the same time. In this blog I'm not going to try and sort out transactions. The error handling
here will be 'if it goes wrong, it goes wrong'. 

In order to do this insertion we are going to have to refactor the batchDataToDatabase method into two parts. We are going to be doing two batchInserts at the same time, and
we want to be able to reuse as much of the code as possible.

The starting code is 

{% highlight scala %}
  protected def batchDataToDatabase(tableName: String, columnNames: List[String], 
      data: Iterator[List[Any]], chunkSize: Int = 10000)(implicit ds: DataSource) = {
    val columnsWithCommas = columnNames.mkString(",")
    val questionMarks = columnNames.map(_ => "?").mkString(",")
    val sql = s"insert into $tableName ($columnsWithCommas) values ($questionMarks)"
    withPreparedStatement(sql, implicit statement => {
      for (chunk <- data.sliding(chunkSize, chunkSize)) {
        for { list <- chunk } {
          for { (value, index) <- list.zipWithIndex }
            statement.setObject(index + 1, value)
          statement.addBatch()
        }
        statement.executeBatch()
      }
    })
  }
  {% endhighlight %}
  Looking at it we can easily break it up into two halves:
  {% highlight scala %}
    protected def batchDataToDatabase(tableName: String, columnNames: List[String], 
         data: Iterator[List[Any]], chunkSize: Int = 10000)(
         implicit ds: DataSource) = {
    withConnection(implicit connection =>
      withStatementForInsert(tableName, columnNames, chunkSize) { 
	      statement =>
	        for (chunk <- data.sliding(chunkSize, chunkSize)) {
	          for { list <- chunk } {
	            for { (value, index) <- list.zipWithIndex }
	              statement.setObject(index + 1, value)
	            statement.addBatch()
	          }
	          statement.executeBatch()
	        }
	      })
      
  def withStatementForInsert(tableName: String, columnNames: List[String], 
                               chunkSize: Int = 10000)(
                               statementFn: PreparedStatement => Unit)(
                               implicit connection: Connection) = {
    val columnsWithCommas = columnNames.mkString(",")
    val questionMarks = columnNames.map(_ => "?").mkString(",")
    val sql = s"insert into $tableName ($columnsWithCommas) values ($questionMarks)"
    val statement = connection.prepareStatement(sql)
    try statementFn(statement) finally statement.close
  }
{% endhighlight %}
This allows us to reuse the 'withStatementForInsert' in order to create two statements: one for the services table, and one for the serviceSegments. That code looks like: 

{% highlight scala %}
  def withStatementsForInsert(defn: MasterSlaveDefn)(
    statementFn: (PreparedStatement, PreparedStatement) => Unit)(
                 implicit dataSource: DataSource) = {
    import defn._
    withConnection { implicit connection =>
      withStatementForInsert(tableName1, columnNames1) { statement1 =>
        withStatementForInsert(tableName2, columnNames2) { statement2 =>
          statementFn(statement1, statement2)
        }
      }
    }
  }
{% endhighlight %}
That's a lot of plumbing! It creates a prepared statement for the master, another for the slave, and deals with all the 'making sure they are closed'. It also allows us to think about
just the plumbing separated from the 'manipulating the tables' code. That 'manipulating the tables' code looks like this
{% highlight scala %}
  def masterSlaveInsert(defn: MasterSlaveDefn, data: Iterator[MasterSlaveData], 
                        chunkSize: Int = 10000)(implicit ds: DataSource) =
    withStatementsForInsert(defn) { (master, slave) =>
      for (chunk <- data.sliding(chunkSize, chunkSize)) {
        for { masterSlaveData <- chunk } {
          addToStatement(master, masterSlaveData.masterData)
          for (s <- masterSlaveData.slaveData)
            addToStatement(slave, s)
        }
        master.executeBatch()
        slave.executeBatch()
      }
    }
{% endhighlight %}
#The final code
{% highlight scala %}
    val dateFormatter = new SimpleDateFormat("yyyy-MM-dd")
    val rawLines = Source.fromFile(file).getLines()
    val scheduleLines = rawLines.filter(_.contains("JsonScheduleV1")).
       map(Json.parse(_) \ "JsonScheduleV1")
    val scheduleData = scheduleLines.zipWithIndex.map {
      case (json, serviceId) =>
        def s(j: JsLookupResult) = j.asOpt[String].getOrElse(null)
        def d(j: JsLookupResult)= new java.sql.Date(dateFormatter.parse(s(j)).getTime)
        val schedule = json \ "schedule_segment"
        val trainUid = s(json \ "CIF_train_uid")
        val trainCategory = s(schedule \ "CIF_train_category")
        val trainServiceCode = s(schedule \ "CIF_train_service_code")
        val scheduleDayRuns = s(json \ "schedule_days_runs")
        val scheduleStartDate = d(json \ "schedule_start_date")
        val scheduleEndDate = d(json \ "schedule_end_date")

        val serviceSegmentData = (schedule \ "schedule_location").as[Seq[JsObject]].
           map { json =>
	          val tiploc = (json \ "tiploc_code").as[String]
	          val departure = s(json \ "departure")
	          val arrival = s(json \ "arrival")
	          List(serviceId, tiploc, departure, arrival)
	        }
        new MasterSlaveData(List(serviceId, trainUid, trainCategory, trainServiceCode,
          scheduleDayRuns, scheduleStartDate, scheduleEndDate), serviceSegmentData)
    }

    val masterSlaveDefn = new MasterSlaveDefn(
      "service_for_blog",
      List("id", "uid", "train_category", "train_service_code", "scheduleDayRuns", 
            "schedule_start_date", "schedule_end_date"),
      "service_segment_for_blob",
      List("service_id", "tiploc", "departure", "arrival"))

    withDatasource { implicit dataSource =>
      masterSlaveInsert(masterSlaveDefn, scheduleData)
    }
  }
{% endhighlight %}
#Summary
I'm not unhappy with it. Most of the volumn of the code is about 'how do I get the data out of JSON'. I'm pleased that the code is only processed once, and it's quite adjustable. I have a number
of other 'load a load of data from a file and write it to a database'. Those other files are not JSON, but the database side doesn't really care about that: it just deals with List[Any].

It still runs slow, taking over three minutes to run.  Even with the batch insert code it takes a long time to parse all the JSON and insert the results into the database. My performance monitor shows that I am only using a couple of cores 
heavily though: one being the JVM, the other being the database. The  next blog we will look at adding some parallelism using Apache Spark
